workdir: config["workdir"]
def remove_clusterlog(): shell('if [ -d "clusterlog/" ]; then if [ ! "$(ls -A clusterlog/)" ]; then rm -rf clusterlog/; fi; fi')
onstart:
	shell("mkdir -p clusterlog/")
onsuccess: 
	remove_clusterlog()
onerror:
	remove_clusterlog()

localrules: all, ganon_check, krakenuniq_prepare, krakenuniq_check, kraken_prepare, kraken_check, centrifuge_prepare, centrifuge_check, diamond_prepare, diamond_check, stats

rule all:
	input:
		stats = [database +"/"+ tool +"_db/"+ parameter + "/" + tool + "_db.stats" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for parameter in config["run"][tool][database]]

rule stats:
	input: time = "{database}/{tool}_db/{parameters}/{tool}_db.time",
		   check = "{database}/{tool}_db/{parameters}/{tool}_db.check"
	output: stats = "{database}/{tool}_db/{parameters}/{tool}_db.stats"
	shell: 
		"""
		#######
		# .stats: tool <tab> database <tab> parameters <tab> time elapsed <tab> time seconds <tab> peak memory bytes <tab> index size bytes
		######

		# elapsed time
		timeelapsed=$(grep -oP "(?<=Elapsed \\(wall clock\\) time \\(h:mm:ss or m:ss\\): ).+" {input.time})
		# time in seconds
		timesec=$(echo ${{timeelapsed}} | awk '{{split($0,a,":");if(length(a)==2){{sec=(a[1]*60)+a[2]}}else{{sec=(a[1]*3600)+(a[2]*60)+a[3]}};print sec }}')

		# peak memory in kilobytes 
		memkbytes=$(grep -oP "(?<=Maximum resident set size \\(kbytes\\): ).+" {input.time})
		# peak memory in bytes
		membytes=$((memkbytes*1000))
		
		# index size (in bytes)
		indexbytes=$(awk -F "\\t" '{{s+=$1}}END{{print s}}' {input.check})

		echo "{wildcards.tool}\t{wildcards.database}\t{wildcards.parameters}\t${{timeelapsed}}\t${{timesec}}\t${{membytes}}\t${{indexbytes}}" > {output.stats}
		"""
		
rule ganon:
	input: fasta = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["fasta"]),
			nodes = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["nodes"]),
			merged = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["merged"]),
			names = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["names"]),
			acc_len_taxid_assembly = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["acc_len_taxid_assembly"])
	output: db1="{database}/ganon_db/{parameters}/ganon_db.bins",
			db2="{database}/ganon_db/{parameters}/ganon_db.filter",
			db3="{database}/ganon_db/{parameters}/ganon_db.map",
			db4="{database}/ganon_db/{parameters}/ganon_db.nodes",
			time="{database}/ganon_db/{parameters}/ganon_db.time",
	log: "{database}/ganon_db/{parameters}/ganon_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/ganon.yaml") 
	params: params = lambda wildcards: config["run"]["ganon"][wildcards.database][wildcards.parameters],
			dbprefix = "{database}/ganon_db/{parameters}/"
	shell: "/usr/bin/time -v --output={output.time} {config[tools_path][ganon]}ganon build -d {params.dbprefix}ganon_db -i {input.fasta} -t {threads} --taxdump-file {input.nodes} {input.names} {input.merged} --seq-info-file {input.acc_len_taxid_assembly} {params.params} > {log} 2>&1"

rule ganon_check:
	input: "{database}/ganon_db/{parameters}/ganon_db.bins",
			"{database}/ganon_db/{parameters}/ganon_db.filter",
			"{database}/ganon_db/{parameters}/ganon_db.map",
			"{database}/ganon_db/{parameters}/ganon_db.nodes"
	output: "{database}/ganon_db/{parameters}/ganon_db.check"
	shell: "du --block-size=1 {input} > {output}" #output in bytes

rule krakenuniq_prepare:
	input: fasta = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["fasta"]),
			acc_len_taxid_assembly = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["acc_len_taxid_assembly"]),
			nodes = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["nodes"]),
			names = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["names"])
	output: fasta = temp("{database}/krakenuniq_db/{parameters}/library/file.fna"),
			seqid2taxid_assembly = temp("{database}/krakenuniq_db/{parameters}/library/seqid2taxid.map"),
			nodes = temp("{database}/krakenuniq_db/{parameters}/taxonomy/nodes.dmp"),
			names = temp("{database}/krakenuniq_db/{parameters}/taxonomy/names.dmp")
	params: dbprefix = "{database}/krakenuniq_db/{parameters}/"
	shell: 
		"""
		mkdir -p {params.dbprefix}taxonomy
		mkdir -p {params.dbprefix}library

		ln -sf {input.nodes} {output.nodes} # link because just use for building (all info in taxDB)
		ln -sf {input.names} {output.names} # link because just use for building (all info in taxDB)
		cp {input.fasta} {output.fasta} # temp copy to get correct jellyfish hash size when building
		awk -F "\\t" 'BEGIN {{FS="\\t";OFS="\\t"}}{{ if($4!="na"){{ print $1,$3,$4 }}}}' {input.acc_len_taxid_assembly} > {output.seqid2taxid_assembly}
		"""


rule krakenuniq:
 	input: fasta = "{database}/krakenuniq_db/{parameters}/library/file.fna",
 			seqid2taxid_assembly = "{database}/krakenuniq_db/{parameters}/library/seqid2taxid.map",
 			nodes = "{database}/krakenuniq_db/{parameters}/taxonomy/nodes.dmp",
			names = "{database}/krakenuniq_db/{parameters}/taxonomy/names.dmp"
	output: db1="{database}/krakenuniq_db/{parameters}/database.idx",
			db2="{database}/krakenuniq_db/{parameters}/database.kdb",
			db3="{database}/krakenuniq_db/{parameters}/taxDB",
			time="{database}/krakenuniq_db/{parameters}/krakenuniq_db.time"
	log: "{database}/krakenuniq_db/{parameters}/krakenuniq_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/krakenuniq.yaml") 
	params: params = lambda wildcards: config["run"]["krakenuniq"][wildcards.database][wildcards.parameters],
			dbprefix = "{database}/krakenuniq_db/{parameters}/"
	shell: 
		"""
		/usr/bin/time -v --output={output.time} {config[tools_path][krakenuniq]}krakenuniq-build --db {params.dbprefix} --threads {threads} {params.params} > {log} 2>&1
		rm -rvf {params.dbprefix}library/ {params.dbprefix}taxonomy/ {params.dbprefix}database_* {params.dbprefix}database.jdb {params.dbprefix}database.kdb.counts {params.dbprefix}database.kraken.tsv {params.dbprefix}database.report.tsv {params.dbprefix}database0.kdb {params.dbprefix}library-files.txt {params.dbprefix}seqid2taxid.map {params.dbprefix}seqid2taxid.map.orig {params.dbprefix}seqid2taxid-plus.map >> {log} 2>&1
		"""

rule krakenuniq_check:
	input: "{database}/krakenuniq_db/{parameters}/database.idx",
			"{database}/krakenuniq_db/{parameters}/database.kdb",
			"{database}/krakenuniq_db/{parameters}/taxDB"
	output: "{database}/krakenuniq_db/{parameters}/krakenuniq_db.check"
	shell: "du --block-size=1 {input} > {output}"#output in bytes

rule kraken_prepare:
	input: acc_len_taxid_assembly = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["acc_len_taxid_assembly"]),
			nodes = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["nodes"]),
			names = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["names"])
	output: accession2taxid = temp("{database}/kraken_db/{parameters}/taxonomy/kraken.accession2taxid"),
			nodes = "{database}/kraken_db/{parameters}/taxonomy/nodes.dmp",
			names = "{database}/kraken_db/{parameters}/taxonomy/names.dmp"
	params: dbprefix = "{database}/kraken_db/{parameters}/"
	shell: 
		"""
		mkdir -p {params.dbprefix}taxonomy
		mkdir -p {params.dbprefix}library

		cp {input.nodes} {output.nodes} # copy because it's part of the db
		cp {input.names} {output.names} # copy because it's part of the db

		awk -F "\\t" 'BEGIN {{FS="\\t";OFS="\\t"; print "accession","accession.version","taxid","gi"}}{{ if($1!="na"){{ split($1,acc,"."); print acc[1],$1,$3,"0" }}}}' {input.acc_len_taxid_assembly} > {output.accession2taxid}
		"""

rule kraken:
 	input: fasta = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["fasta"]),
 			accession2taxid = "{database}/kraken_db/{parameters}/taxonomy/kraken.accession2taxid",
 			nodes = "{database}/kraken_db/{parameters}/taxonomy/nodes.dmp",
			names = "{database}/kraken_db/{parameters}/taxonomy/names.dmp"
	output: db1="{database}/kraken_db/{parameters}/database.idx",
			db2="{database}/kraken_db/{parameters}/database.kdb",
			time="{database}/kraken_db/{parameters}/kraken_db.time"
	log: "{database}/kraken_db/{parameters}/kraken_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/kraken.yaml") 
	params: params = lambda wildcards: config["run"]["kraken"][wildcards.database][wildcards.parameters],
			dbprefix = "{database}/kraken_db/{parameters}/"
	shell: 
		"""
		/usr/bin/time -v --output={output.time} bash -c "
		{config[tools_path][kraken]}kraken-build --db {params.dbprefix} --add-to-library {input.fasta} > {log} 2>&1;
		{config[tools_path][kraken]}kraken-build --build --db {params.dbprefix} --threads {threads} {params.params} >> {log} 2>&1
		"
		rm -rfv {params.dbprefix}library/ {params.dbprefix}database.jdb {params.dbprefix}accmap_file.tmp {params.dbprefix}lca.complete {params.dbprefix}seqid2taxid.map {params.dbprefix}taxonomy/prelim_map.txt >> {log} 2>&1
		"""

rule kraken_check:
	input: "{database}/kraken_db/{parameters}/database.idx",
		"{database}/kraken_db/{parameters}/database.kdb",
		"{database}/kraken_db/{parameters}/taxonomy/nodes.dmp",
		"{database}/kraken_db/{parameters}/taxonomy/names.dmp"
	output: "{database}/kraken_db/{parameters}/kraken_db.check"
	shell: "du --block-size=1 {input} > {output}"#output in bytes

rule centrifuge_prepare:
	input: acc_len_taxid_assembly = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["acc_len_taxid_assembly"])
	output: seqid2taxid = temp("{database}/centrifuge_db/{parameters}/seqid2taxid.map")
	shell:
		"""
		awk -F "\\t" 'BEGIN {{FS="\\t";OFS="\\t"}}{{ if($4!="na"){{ print $1,$3}} }}' {input.acc_len_taxid_assembly} > {output.seqid2taxid}
		"""

rule centrifuge:
 	input: fasta = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["fasta"]),
 			seqid2taxid = "{database}/centrifuge_db/{parameters}/seqid2taxid.map",
 			nodes = lambda wildcards: config["databases"][wildcards.database]["nodes"],
			names = lambda wildcards: config["databases"][wildcards.database]["names"]
	output: db1="{database}/centrifuge_db/{parameters}/centrifuge_db.1.cf",
			db2="{database}/centrifuge_db/{parameters}/centrifuge_db.2.cf",
			db3="{database}/centrifuge_db/{parameters}/centrifuge_db.3.cf",
			time="{database}/centrifuge_db/{parameters}/centrifuge_db.time"
	log: "{database}/centrifuge_db/{parameters}/centrifuge_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/centrifuge.yaml") 
	params: params = lambda wildcards: config["run"]["centrifuge"][wildcards.database][wildcards.parameters],
			dbprefix = "{database}/centrifuge_db/{parameters}/"
	shell: 
		"/usr/bin/time -v --output={output.time} {config[tools_path][centrifuge]}centrifuge-build -p {threads} {params.params} --taxonomy-tree {input.nodes} --name-table {input.names} --conversion-table {input.seqid2taxid} {input.fasta} {params.dbprefix}centrifuge_db > {log} 2>&1"

rule centrifuge_check:
	input:  "{database}/centrifuge_db/{parameters}/centrifuge_db.1.cf",
			"{database}/centrifuge_db/{parameters}/centrifuge_db.2.cf",
			"{database}/centrifuge_db/{parameters}/centrifuge_db.3.cf"
	output: "{database}/centrifuge_db/{parameters}/centrifuge_db.check"
	shell: "du --block-size=1 {input} > {output}"#output in bytes

rule diamond_prepare:
	output: accession2taxid = "{database}/prot.accession2taxid.gz"
	params: dbprefix = "{database}/"
	shell: 
		"""
		mkdir -p {params.dbprefix}
		curl -L --silent -o {output.accession2taxid}  ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.gz
		"""

rule diamond:
 	input: faa = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["faa"]),
 			accession2taxid = "{database}/prot.accession2taxid.gz",
 			nodes = lambda wildcards: config["databases"][wildcards.database]["nodes"]
	output: db1="{database}/diamond_db/{parameters}/diamond_db.dmnd",
			time="{database}/diamond_db/{parameters}/diamond_db.time"
	log: "{database}/diamond_db/{parameters}/diamond_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/diamond.yaml") 
	params: params = lambda wildcards: config["run"]["diamond"][wildcards.database][wildcards.parameters],
			dbprefix = "{database}/diamond_db/{parameters}/"
	shell: "/usr/bin/time -v --output={output.time} {config[tools_path][diamond]}diamond makedb --db {params.dbprefix}diamond_db --in {input.faa} --threads {threads} --taxonmap {input.accession2taxid} --taxonnodes {input.nodes} > {log} 2>&1"

rule diamond_check:
	input: "{database}/diamond_db/{parameters}/diamond_db.dmnd"
	output: "{database}/diamond_db/{parameters}/diamond_db.check"
	shell: "du --block-size=1 {input} > {output}"#output in bytes

