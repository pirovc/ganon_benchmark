workdir: config["workdir"]
def remove_clusterlog(): shell('if [ -d "clusterlog/" ]; then if [ ! "$(ls -A clusterlog/)" ]; then rm -rf clusterlog/; fi; fi')
onstart:
	shell("mkdir -p clusterlog/")
onsuccess: 
	remove_clusterlog()
onerror:
	remove_clusterlog()

localrules: all, ganon_check, krakenuniq_check, kraken_check, centrifuge_check, stats

rule all:
	input:
		stats = [database +"/"+ tool +"_db/"+ parameter + "/" + tool + "_db.stats" for database in config["databases"] for tool in config["tools"] for parameter in config["parameters"][tool]]

rule stats:
	input: time = "{database}/ganon_db/{parameters}/{tool}_db.time",
		   check = "{database}/ganon_db/{parameters}/{tool}_db.check"
	output: stats = "{database}/ganon_db/{parameters}/{tool}_db.stats"
	shell: 
	"""
		awk '{s+=$1}END{print s}' {input.check} > {output.stats} #sum of the file sizes
		grep -oP "(?<=Elapsed \(wall clock\) time \(h:mm:ss or m:ss\):)[0-9:]*" {input.time} >> {output.stats}
		grep -oP "(?<=Maximum resident set size \(kbytes\): ).+" {input.time} >> {output.stats}
	"""

rule ganon:
	input: fasta = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["fasta"])
	output: db1="{database}/ganon_db/{parameters}/ganon_db.bins",
			db2="{database}/ganon_db/{parameters}/ganon_db.filter",
			db3="{database}/ganon_db/{parameters}/ganon_db.map",
			db4="{database}/ganon_db/{parameters}/ganon_db.nodes",
			time="{database}/ganon_db/{parameters}/ganon_db.time",
	log: "{database}/ganon_db/{parameters}/ganon_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/ganon.yaml") 
	params: params = lambda wildcards: config["parameters"]["ganon"][wildcards.parameters],
			nodes = lambda wildcards: config["databases"][wildcards.database]["nodes"],
			merged = lambda wildcards: config["databases"][wildcards.database]["merged"],
			names = lambda wildcards: config["databases"][wildcards.database]["names"],
			lentaxid = lambda wildcards: config["databases"][wildcards.database]["lentaxid"]
	shell: "/usr/bin/time -v --output={output.time} {config[tools][ganon]}ganon build -d {wildcards.database}/ganon_db/{wildcards.parameters}/ganon_db -i {input.fasta} -t {threads} --taxdump-file {params.nodes} {params.names} {params.merged} --seq-info-file {params.lentaxid} {params.params} > {log} 2>&1"

rule ganon_check:
	input: "{database}/ganon_db/{parameters}/ganon_db.bins",
			"{database}/ganon_db/{parameters}/ganon_db.filter",
			"{database}/ganon_db/{parameters}/ganon_db.map",
			"{database}/ganon_db/{parameters}/ganon_db.nodes"
	output: "{database}/ganon_db/{parameters}/ganon_db.check"
	shell: "du -b {input} > {output}"

rule krakenuniq:
 	input: fasta = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["fasta"])
	output: db1="{database}/krakenuniq_db/{parameters}/database.idx",
			db2="{database}/krakenuniq_db/{parameters}/database.kdb",
			db3="{database}/krakenuniq_db/{parameters}/taxDB",
			time="{database}/krakenuniq_db/{parameters}/krakenuniq_db.time"
	log: "{database}/krakenuniq_db/{parameters}/krakenuniq_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/krakenuniq.yaml") 
	params: params = lambda wildcards: config["parameters"]["krakenuniq"][wildcards.parameters],
			nodes = lambda wildcards: config["databases"][wildcards.database]["nodes"],
			names = lambda wildcards: config["databases"][wildcards.database]["names"],
			seqid2taxid_assembly = lambda wildcards: config["databases"][wildcards.database]["seqid2taxid_assembly"]
	shell: 
		"""
		dbprefix={wildcards.database}/krakenuniq_db/{wildcards.parameters}/
		mkdir -p ${{dbprefix}}taxonomy
		ln -sf {params.nodes} ${{dbprefix}}taxonomy/nodes.dmp
		ln -sf {params.names} ${{dbprefix}}taxonomy/names.dmp

		mkdir -p ${{dbprefix}}library
		cp {input.fasta} ${{dbprefix}}library/file.fna #copy to get jellyfish hash size
		ln -sf  {params.seqid2taxid_assembly} ${{dbprefix}}library/seqid2taxid.map

		/usr/bin/time -v --output={output.time} {config[tools][krakenuniq]}krakenuniq-build --db ${{dbprefix}} --threads {threads} {params.params} > {log} 2>&1
		rm -rvf ${{dbprefix}}library/ ${{dbprefix}}taxonomy/ ${{dbprefix}}database_* ${{dbprefix}}database.jdb ${{dbprefix}}database.kdb.counts ${{dbprefix}}database.kraken.tsv ${{dbprefix}}database.report.tsv ${{dbprefix}}database0.kdb ${{dbprefix}}library-files.txt ${{dbprefix}}seqid2taxid.map ${{dbprefix}}seqid2taxid.map.orig ${{dbprefix}}seqid2taxid-plus.map >> {log} 2>&1
		"""

rule krakenuniq_check:
	input: "{database}/krakenuniq_db/{parameters}/database.idx",
			"{database}/krakenuniq_db/{parameters}/database.kdb",
			"{database}/krakenuniq_db/{parameters}/taxDB"
	output: "{database}/krakenuniq_db/{parameters}/krakenuniq_db.check"
	shell: "du -b {input} > {output}"

rule kraken:
 	input: fasta = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["fasta"])
	output: db1="{database}/kraken_db/{parameters}/database.idx",
			db2="{database}/kraken_db/{parameters}/database.kdb",
			db3=directory("{database}/kraken_db/{parameters}/taxonomy/"),
			time="{database}/kraken_db/{parameters}/kraken_db.time"
	log: "{database}/kraken_db/{parameters}/kraken_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/kraken.yaml") 
	params: params = lambda wildcards: config["parameters"]["kraken"][wildcards.parameters],
			nodes = lambda wildcards: config["databases"][wildcards.database]["nodes"],
			names = lambda wildcards: config["databases"][wildcards.database]["names"],
			accession2taxid = lambda wildcards: config["databases"][wildcards.database]["accession2taxid"]
	shell: 
		"""
		dbprefix="{wildcards.database}/kraken_db/{wildcards.parameters}/"
		mkdir -p ${{dbprefix}}taxonomy
		ln -sf {params.nodes} ${{dbprefix}}taxonomy/nodes.dmp
		ln -sf {params.names} ${{dbprefix}}taxonomy/names.dmp
		ln -sf {params.accession2taxid} ${{dbprefix}}taxonomy/kraken.accession2taxid
	
		/usr/bin/time -v --output={output.time} bash -c "
		{config[tools][kraken]}kraken-build --db ${{dbprefix}} --add-to-library {input.fasta} > {log} 2>&1;
		{config[tools][kraken]}kraken-build --build --db ${{dbprefix}} --threads {threads} {params.params} >> {log} 2>&1
		"

		rm -rfv ${{dbprefix}}library/ ${{dbprefix}}database.jdb ${{dbprefix}}accmap_file.tmp ${{dbprefix}}lca.complete ${{dbprefix}}seqid2taxid.map >> {log} 2>&1
		"""

rule kraken_check:
	input: "{database}/kraken_db/{parameters}/database.idx",
		"{database}/kraken_db/{parameters}/database.kdb",
		"{database}/kraken_db/{parameters}/taxonomy/"
	output: "{database}/kraken_db/{parameters}/kraken_db.check"
	shell: "du -b {input} > {output}"

rule centrifuge:
 	input: fasta = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["fasta"])
	output: db1="{database}/centrifuge_db/{parameters}/centrifuge_db.1.cf",
			db2="{database}/centrifuge_db/{parameters}/centrifuge_db.2.cf",
			db3="{database}/centrifuge_db/{parameters}/centrifuge_db.3.cf",
			time="{database}/centrifuge_db/{parameters}/centrifuge_db.time"
	log: "{database}/centrifuge_db/{parameters}/centrifuge_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/centrifuge.yaml") 
	params: params = lambda wildcards: config["parameters"]["centrifuge"][wildcards.parameters],
			nodes = lambda wildcards: config["databases"][wildcards.database]["nodes"],
			names = lambda wildcards: config["databases"][wildcards.database]["names"],
			seqid2taxid = lambda wildcards: config["databases"][wildcards.database]["seqid2taxid"]
	shell: 
		"/usr/bin/time -v --output={output.time} {config[tools][centrifuge]}centrifuge-build -p {threads} {params.params} --taxonomy-tree {params.nodes} --name-table {params.names} --conversion-table {params.seqid2taxid} {input.fasta} {wildcards.database}/centrifuge_db/{wildcards.parameters}/centrifuge_db > {log} 2>&1"

rule centrifuge_check:
	input:  "{database}/centrifuge_db/{parameters}/centrifuge_db.1.cf",
			"{database}/centrifuge_db/{parameters}/centrifuge_db.2.cf",
			"{database}/centrifuge_db/{parameters}/centrifuge_db.3.cf"
	output: "{database}/centrifuge_db/{parameters}/centrifuge_db.check"
	shell: "du -b {input} > {output}"