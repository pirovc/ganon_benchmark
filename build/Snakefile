workdir: config["workdir"]
def remove_clusterlog(): shell('if [ -d "clusterlog/" ]; then if [ ! "$(ls -A clusterlog/)" ]; then rm -rf clusterlog/; fi; fi')
onstart:
	shell("mkdir -p clusterlog/")
onsuccess: 
	remove_clusterlog()
onerror:
	remove_clusterlog()

localrules: all, ganon_check, krakenuniq_prepare, krakenuniq_check, kraken_prepare, kraken_check, centrifuge_prepare, centrifuge_check, diamond_prepare, diamond_check, clark_prepare, clark_check, stats

rule all:
	input:
		stats = [database +"/"+ tool +"_db/"+ parameter + "/" + tool + "_db.stats" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for parameter in config["run"][tool][database]]

rule stats:
	input: time = "{database}/{tool}_db/{parameters}/{tool}_db.time",
		   check = "{database}/{tool}_db/{parameters}/{tool}_db.check"
	output: stats = "{database}/{tool}_db/{parameters}/{tool}_db.stats"
	shell: 
		"""
		#######
		# .stats: tool <tab> database <tab> parameters <tab> time elapsed <tab> time seconds <tab> peak memory bytes <tab> index size bytes
		######

		# elapsed time
		timeelapsed=$(grep -oP "(?<=Elapsed \\(wall clock\\) time \\(h:mm:ss or m:ss\\): ).+" {input.time})
		# time in seconds
		timesec=$(echo ${{timeelapsed}} | 
		awk '{{l=split($0,a,":");
		if(l==2){{
			sec=(a[1]*60)+a[2];
		}}else{{
			sec=(a[1]*3600)+(a[2]*60)+a[3]
		}};
		print sec }}')

		# peak memory in kilobytes 
		memkbytes=$(grep -oP "(?<=Maximum resident set size \\(kbytes\\): ).+" {input.time})
		# peak memory in bytes
		membytes=$((memkbytes*1000))
		
		# index size (in bytes)
		indexbytes=$(awk -F "\\t" '{{s+=$1}}END{{print s}}' {input.check})

		echo "{wildcards.tool}\t{wildcards.database}\t{wildcards.parameters}\t${{timeelapsed}}\t${{timesec}}\t${{membytes}}\t${{indexbytes}}" > {output.stats}
		"""
		
rule ganon:
	input: fasta = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["fasta"]),
			nodes = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["nodes"]),
			merged = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["merged"]),
			names = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["names"]),
			acc_len_taxid_assembly = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["acc_len_taxid_assembly"])
	output: db1="{database}/ganon_db/{parameters}/ganon_db.ibf",
			db2="{database}/ganon_db/{parameters}/ganon_db.tax",
			db3="{database}/ganon_db/{parameters}/ganon_db.map",
			db4="{database}/ganon_db/{parameters}/ganon_db.gnn",
			time="{database}/ganon_db/{parameters}/ganon_db.time",
	log: "{database}/ganon_db/{parameters}/ganon_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/ganon.yaml") 
	params: params = lambda wildcards: config["run"]["ganon"][wildcards.database][wildcards.parameters],
			dbprefix = "{database}/ganon_db/{parameters}/"
	shell: "/usr/bin/time -v --output={output.time} {config[tools_path][ganon]}ganon build --db-prefix {params.dbprefix}ganon_db --input-files {input.fasta} --threads {threads} --taxdump-file {input.nodes} {input.names} {input.merged} --seq-info-file {input.acc_len_taxid_assembly} {params.params} > {log} 2>&1"

rule ganon_check:
	input: "{database}/ganon_db/{parameters}/ganon_db.ibf",
			"{database}/ganon_db/{parameters}/ganon_db.tax",
			"{database}/ganon_db/{parameters}/ganon_db.map",
			"{database}/ganon_db/{parameters}/ganon_db.gnn"
	output: "{database}/ganon_db/{parameters}/ganon_db.check"
	shell: "du --block-size=1 {input} > {output}" #output in bytes

rule krakenuniq_prepare:
	input: fasta = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["fasta"]),
			acc_len_taxid_assembly = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["acc_len_taxid_assembly"]),
			nodes = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["nodes"]),
			names = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["names"])
	output: fasta = temp("{database}/krakenuniq_db/{parameters}/library/file.fna"),
			seqid2taxid_assembly = temp("{database}/krakenuniq_db/{parameters}/library/seqid2taxid.map"),
			nodes = temp("{database}/krakenuniq_db/{parameters}/taxonomy/nodes.dmp"),
			names = temp("{database}/krakenuniq_db/{parameters}/taxonomy/names.dmp")
	params: dbprefix = "{database}/krakenuniq_db/{parameters}/"
	shell: 
		"""
		mkdir -p {params.dbprefix}taxonomy
		mkdir -p {params.dbprefix}library

		ln -sf {input.nodes} {output.nodes} # link because just use for building (all info in taxDB)
		ln -sf {input.names} {output.names} # link because just use for building (all info in taxDB)
		cp {input.fasta} {output.fasta} # temp copy to get correct jellyfish hash size when building
		awk -F "\\t" 'BEGIN {{FS="\\t";OFS="\\t"}}{{ if($4!="na"){{ print $1,$3,$4 }}}}' {input.acc_len_taxid_assembly} > {output.seqid2taxid_assembly}
		"""

rule krakenuniq:
 	input: fasta = "{database}/krakenuniq_db/{parameters}/library/file.fna",
 			seqid2taxid_assembly = "{database}/krakenuniq_db/{parameters}/library/seqid2taxid.map",
 			nodes = "{database}/krakenuniq_db/{parameters}/taxonomy/nodes.dmp",
			names = "{database}/krakenuniq_db/{parameters}/taxonomy/names.dmp"
	output: db1="{database}/krakenuniq_db/{parameters}/database.idx",
			db2="{database}/krakenuniq_db/{parameters}/database.kdb",
			db3="{database}/krakenuniq_db/{parameters}/taxDB",
			time="{database}/krakenuniq_db/{parameters}/krakenuniq_db.time"
	log: "{database}/krakenuniq_db/{parameters}/krakenuniq_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/krakenuniq.yaml") 
	params: params = lambda wildcards: config["run"]["krakenuniq"][wildcards.database][wildcards.parameters],
			dbprefix = "{database}/krakenuniq_db/{parameters}/"
	shell: 
		"""
		/usr/bin/time -v --output={output.time} {config[tools_path][krakenuniq]}krakenuniq-build --db {params.dbprefix} --threads {threads} {params.params} > {log} 2>&1
		rm -rvf {params.dbprefix}library/ {params.dbprefix}taxonomy/ {params.dbprefix}database_* {params.dbprefix}database.jdb {params.dbprefix}database.kdb.counts {params.dbprefix}database.kraken.tsv {params.dbprefix}database.report.tsv {params.dbprefix}database0.kdb {params.dbprefix}library-files.txt {params.dbprefix}seqid2taxid.map {params.dbprefix}seqid2taxid.map.orig {params.dbprefix}seqid2taxid-plus.map >> {log} 2>&1
		"""

rule krakenuniq_check:
	input: "{database}/krakenuniq_db/{parameters}/database.idx",
			"{database}/krakenuniq_db/{parameters}/database.kdb",
			"{database}/krakenuniq_db/{parameters}/taxDB"
	output: "{database}/krakenuniq_db/{parameters}/krakenuniq_db.check"
	shell: "du --block-size=1 {input} > {output}"#output in bytes

rule kraken_prepare:
	input: acc_len_taxid_assembly = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["acc_len_taxid_assembly"]),
			nodes = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["nodes"]),
			names = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["names"])
	output: accession2taxid = temp("{database}/kraken_db/{parameters}/taxonomy/kraken.accession2taxid"),
			nodes = "{database}/kraken_db/{parameters}/taxonomy/nodes.dmp",
			names = "{database}/kraken_db/{parameters}/taxonomy/names.dmp"
	params: dbprefix = "{database}/kraken_db/{parameters}/"
	shell: 
		"""
		mkdir -p {params.dbprefix}taxonomy
		mkdir -p {params.dbprefix}library

		cp {input.nodes} {output.nodes} # copy because it's part of the db
		cp {input.names} {output.names} # copy because it's part of the db

		awk -F "\\t" 'BEGIN {{FS="\\t";OFS="\\t"; print "accession","accession.version","taxid","gi"}}{{ if($1!="na"){{ split($1,acc,"."); print acc[1],$1,$3,"0" }}}}' {input.acc_len_taxid_assembly} > {output.accession2taxid}
		"""

rule kraken:
 	input: fasta = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["fasta"]),
 			accession2taxid = "{database}/kraken_db/{parameters}/taxonomy/kraken.accession2taxid",
 			nodes = "{database}/kraken_db/{parameters}/taxonomy/nodes.dmp",
			names = "{database}/kraken_db/{parameters}/taxonomy/names.dmp"
	output: db1="{database}/kraken_db/{parameters}/database.idx",
			db2="{database}/kraken_db/{parameters}/database.kdb",
			time="{database}/kraken_db/{parameters}/kraken_db.time"
	log: "{database}/kraken_db/{parameters}/kraken_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/kraken.yaml") 
	params: params = lambda wildcards: config["run"]["kraken"][wildcards.database][wildcards.parameters],
			dbprefix = "{database}/kraken_db/{parameters}/"
	shell: 
		"""
		/usr/bin/time -v --output={output.time} bash -c "
		{config[tools_path][kraken]}kraken-build --db {params.dbprefix} --add-to-library {input.fasta} > {log} 2>&1;
		{config[tools_path][kraken]}kraken-build --build --db {params.dbprefix} --threads {threads} {params.params} >> {log} 2>&1
		"
		rm -rfv {params.dbprefix}library/ {params.dbprefix}database.jdb {params.dbprefix}accmap_file.tmp {params.dbprefix}lca.complete {params.dbprefix}seqid2taxid.map {params.dbprefix}taxonomy/prelim_map.txt >> {log} 2>&1
		"""

rule kraken_check:
	input: "{database}/kraken_db/{parameters}/database.idx",
		"{database}/kraken_db/{parameters}/database.kdb",
		"{database}/kraken_db/{parameters}/taxonomy/nodes.dmp",
		"{database}/kraken_db/{parameters}/taxonomy/names.dmp"
	output: "{database}/kraken_db/{parameters}/kraken_db.check"
	shell: "du --block-size=1 {input} > {output}"#output in bytes

rule centrifuge_prepare:
	input: acc_len_taxid_assembly = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["acc_len_taxid_assembly"])
	output: seqid2taxid = temp("{database}/centrifuge_db/{parameters}/seqid2taxid.map")
	shell:
		"""
		awk -F "\\t" 'BEGIN {{FS="\\t";OFS="\\t"}}{{ if($4!="na"){{ print $1,$3}} }}' {input.acc_len_taxid_assembly} > {output.seqid2taxid}
		"""

rule centrifuge:
 	input: fasta = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["fasta"]),
 			seqid2taxid = "{database}/centrifuge_db/{parameters}/seqid2taxid.map",
 			nodes = lambda wildcards: config["databases"][wildcards.database]["nodes"],
			names = lambda wildcards: config["databases"][wildcards.database]["names"]
	output: db1="{database}/centrifuge_db/{parameters}/centrifuge_db.1.cf",
			db2="{database}/centrifuge_db/{parameters}/centrifuge_db.2.cf",
			db3="{database}/centrifuge_db/{parameters}/centrifuge_db.3.cf",
			time="{database}/centrifuge_db/{parameters}/centrifuge_db.time"
	log: "{database}/centrifuge_db/{parameters}/centrifuge_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/centrifuge.yaml") 
	params: params = lambda wildcards: config["run"]["centrifuge"][wildcards.database][wildcards.parameters],
			dbprefix = "{database}/centrifuge_db/{parameters}/"
	shell: 
		"/usr/bin/time -v --output={output.time} {config[tools_path][centrifuge]}centrifuge-build -p {threads} {params.params} --taxonomy-tree {input.nodes} --name-table {input.names} --conversion-table {input.seqid2taxid} {input.fasta} {params.dbprefix}centrifuge_db > {log} 2>&1"

rule centrifuge_check:
	input:  "{database}/centrifuge_db/{parameters}/centrifuge_db.1.cf",
			"{database}/centrifuge_db/{parameters}/centrifuge_db.2.cf",
			"{database}/centrifuge_db/{parameters}/centrifuge_db.3.cf"
	output: "{database}/centrifuge_db/{parameters}/centrifuge_db.check"
	shell: "du --block-size=1 {input} > {output}"#output in bytes

rule diamond_prepare:
	output: accession2taxid = "{database}/prot.accession2taxid.gz",
			taxdump = temp("{database}/taxdump.tar.gz"),
			diamond_nodes = "{database}/diamond_nodes.dmp" # needs the most up-to-date taxonomy to match prot.accession2taxid.gz
	params: dbprefix = "{database}/"
	shell: 
		"""
		mkdir -p {params.dbprefix}
		curl -L --silent -o {output.accession2taxid}  ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.gz
		curl -L --silent -o {output.taxdump}  ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz
		tar xf {output.taxdump} -C {params.dbprefix} nodes.dmp
		mv {params.dbprefix}nodes.dmp {params.dbprefix}diamond_nodes.dmp
		"""

rule diamond:
 	input: faa = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["faa"]),
 			accession2taxid = "{database}/prot.accession2taxid.gz",
 			diamond_nodes = "{database}/diamond_nodes.dmp"
	output: db1="{database}/diamond_db/{parameters}/diamond_db.dmnd",
			time="{database}/diamond_db/{parameters}/diamond_db.time"
	log: "{database}/diamond_db/{parameters}/diamond_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/diamond.yaml") 
	params: params = lambda wildcards: config["run"]["diamond"][wildcards.database][wildcards.parameters],
			dbprefix = "{database}/diamond_db/{parameters}/"
	shell: "/usr/bin/time -v --output={output.time} {config[tools_path][diamond]}diamond makedb --db {params.dbprefix}diamond_db --in {input.faa} --threads {threads} --taxonmap {input.accession2taxid} --taxonnodes {input.diamond_nodes} > {log} 2>&1"

rule diamond_check:
	input: "{database}/diamond_db/{parameters}/diamond_db.dmnd"
	output: "{database}/diamond_db/{parameters}/diamond_db.check"
	shell: "du --block-size=1 {input} > {output}" #output in bytes

rule clark_prepare:
	input: 	fasta = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["fasta"]),
			nodes = lambda wildcards: config["databases"][wildcards.database]["nodes"],
			merged = lambda wildcards: config["databases"][wildcards.database]["merged"],
			names = lambda wildcards: config["databases"][wildcards.database]["names"],
			acc_len_taxid_assembly = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["acc_len_taxid_assembly"])
	output: seq_folder = temp(directory("{database}/clark_db/{parameters}/Custom/")),
			nodes = temp("{database}/clark_db/{parameters}/taxonomy/nodes.dmp"),
			names = temp("{database}/clark_db/{parameters}/taxonomy/names.dmp"),
			merged = temp("{database}/clark_db/{parameters}/taxonomy/merged.dmp"),
			nucl_accss = temp("{database}/clark_db/{parameters}/taxonomy/nucl_accss"),
			taxondata = temp(touch("{database}/clark_db/{parameters}/.taxondata")),
			dummy_file = temp("{database}/clark_db/{parameters}/dummy_file.fq")
	params: dbprefix = "{database}/clark_db/{parameters}/"
	shell:
		"""
		# Emulate the script: opt/clark/download_taxondata.sh 
		mkdir -p {params.dbprefix}/taxonomy/
		mkdir -p {params.dbprefix}/Custom/

		# split fasta into single sequences:
		awk -v seq_folder='{output.seq_folder}' '{{if (substr($0, 1, 1)==">") {{close(filename);filename=(seq_folder "/" substr($1,2) ".fna")}}; print $0 > filename;}}' {input.fasta}

		# simulate .accession2taxid files
		awk -F "\\t" 'BEGIN {{FS="\\t";OFS="\\t"; print "accession","accession.version","taxid","gi"}}{{ if($1!="na"){{ split($1,acc,"."); print acc[1],$1,$3,"0" }}}}' {input.acc_len_taxid_assembly} > {output.nucl_accss}

		# link taxdump files
		ln -sf {input.nodes} {output.nodes} # link because just use for building
		ln -sf {input.names} {output.names}  # link because just use for building
		ln -sf {input.merged} {output.merged} # link because just use for building

		# generate dummy fastq
		printf "@read1\nTCTCAGTTATTAGAAATTCAATCCGATTTAGGACGACTAGGCATTTCAAT\n+\n@CBFFF@FFH8HHFIBJIJJJ?CIJGEJJIC)EIIADIJHG(D-DJA&EJ\n@read2\nGCTTCTACTACGGGCTGACGGCGTTAGCGTGCGCATGGTACTTCCGCAAC\n+\nC=C<FFFFHHDHGJ?HIJ<H;JDII)EIHJEGJ@GIIGHIJIIEEIIIHJ" > {output.dummy_file}		
		"""

rule clark:
 	input: seq_folder = "{database}/clark_db/{parameters}/Custom/",
			nodes = "{database}/clark_db/{parameters}/taxonomy/nodes.dmp",
			names = "{database}/clark_db/{parameters}/taxonomy/names.dmp",
			merged = "{database}/clark_db/{parameters}/taxonomy/merged.dmp",
			nucl_accss = "{database}/clark_db/{parameters}/taxonomy/nucl_accss",
			taxondata = "{database}/clark_db/{parameters}/.taxondata",
			dummy_file = "{database}/clark_db/{parameters}/dummy_file.fq"
	output: db_folder = directory("{database}/clark_db/{parameters}/custom_0/"), # Use custom_0/ as input because db output files have variable names
			targets = "{database}/clark_db/{parameters}/targets.txt",
			dummy_out = temp("{database}/clark_db/{parameters}/dummy_out.csv"),
			time="{database}/clark_db/{parameters}/clark_db.time"
	log: "{database}/clark_db/{parameters}/clark_db.log"
	threads: config["threads"]
	conda: srcdir("../envs/clark.yaml") 
	params: params = lambda wildcards: config["run"]["clark"][wildcards.database][wildcards.parameters],
			dbprefix = "{database}/clark_db/{parameters}/"
	shell: 
		"""
		# The script should be on set_targets.sh script folder (ENV/opt/clark/) to generate the database 
		# with correct paths. If there's no alternative path set, get current environment path 
		if [[ "{config[tools_path][clark]}" != "" ]]; then clark_opt_path="{config[tools_path][clark]}"; else clark_opt_path="${{PATH%%/bin:*}}/opt/clark/"; fi
		# store current path
		current_path=$(pwd)
		# Get absolute paths in case they are relative
		touch {log}
		abs_log=$(readlink -e {log})
		abs_dbprefix=$(readlink -e {params.dbprefix})

		# run set_targets and dummy to generate db
		/usr/bin/time -v --output={output.time} bash -c "
		cd ${{clark_opt_path}};
		./set_targets.sh ${{abs_dbprefix}}/ custom > ${{abs_log}} 2>&1;
		cd ${{current_path}};

		{config[tools_path][clark]}CLARK -T {output.targets} -D {params.dbprefix}/custom_0/ -O {input.dummy_file} -R {params.dbprefix}/dummy_out -n {threads} >> {log} 2>&1;
		"
		rm -rfv {params.dbprefix}.custom {params.dbprefix}.custom.fileToAccssnTaxID {params.dbprefix}.custom.fileToTaxIDs {params.dbprefix}.tmp {params.dbprefix}taxonomy/ >> {log} 2>&1
		"""

rule clark_check:
	input: db="{database}/clark_db/{parameters}/custom_0/",
			targets="{database}/clark_db/{parameters}/targets.txt"
	output: "{database}/clark_db/{parameters}/clark_db.check"
	shell: "du --block-size=1 {input} > {output}" #output in bytes