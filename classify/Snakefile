workdir: config["workdir"]
def remove_clusterlog(): shell('if [ -d "clusterlog/" ]; then if [ ! "$(ls -A clusterlog/)" ]; then rm -rf clusterlog/; fi; fi')
onstart:
        shell("mkdir -p clusterlog/")
onsuccess:
        remove_clusterlog()
onerror:
        remove_clusterlog()

rule all:
	input:
		cumu_tsv = [database +"/"+ sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".cumu.tsv" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for dbconfig in config["run"][tool][database] for parameter in config["run"][tool][database][dbconfig]["parameters"] for sample in config["samples"]],
		cumu_npz = [database +"/"+ sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".cumu.npz" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for dbconfig in config["run"][tool][database] for parameter in config["run"][tool][database][dbconfig]["parameters"] for sample in config["samples"]],
		rank_tsv = [database +"/"+ sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".rank.tsv" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for dbconfig in config["run"][tool][database] for parameter in config["run"][tool][database][dbconfig]["parameters"] for sample in config["samples"]],
		rank_npz = [database +"/"+ sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".rank.npz" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for dbconfig in config["run"][tool][database] for parameter in config["run"][tool][database][dbconfig]["parameters"] for sample in config["samples"]],
		stats = [database +"/"+ sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".stats" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for dbconfig in config["run"][tool][database] for parameter in config["run"][tool][database][dbconfig]["parameters"] for sample in config["samples"]],
		amber_outdir = [database +"/"+ sample +"/amber/" for tool in config["run"] for database in config["run"][tool] for sample in config["samples"]]

rule stats:
	input: time = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.time",
			mbpm = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.mbpm"
	output: stats = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.stats"
	shell: 
		"""
		#######
		# .stats: tool <tab> sample <tab> database <tab> dbconfig <tab> parameters <tab> time elapsed <tab> time elapsed seconds <tab> Mbp/m <tab> peak memory bytes
		######

		# elapsed time
		timeelapsed=$(grep -oP "(?<=Elapsed \\(wall clock\\) time \\(h:mm:ss or m:ss\\): ).+" {input.time})
		# time in seconds
		timesec=$(echo ${{timeelapsed}} | awk '{{split($0,a,":");if(length(a)==2){{sec=(a[1]*60)+a[2]}}else{{sec=(a[1]*3600)+(a[2]*60)+a[3]}};print sec }}')

		# peak memory in kilobytes 
		memkbytes=$(grep -oP "(?<=Maximum resident set size \\(kbytes\\): ).+" {input.time})
		# peak memory in bytes
		membytes=$((memkbytes*1000))

		#Mbp/m
		mbpm=$(cat {input.mbpm})

		echo "{wildcards.tool}\t{wildcards.sample}\t{wildcards.database}\t{wildcards.dbconfig}\t{wildcards.parameters}\t${{timeelapsed}}\t${{timesec}}\t${{mbpm}}\t${{membytes}}" > {output.stats}
		"""

rule amber:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			results = lambda wildcards: [wildcards.database +"/"+ wildcards.sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".out.gz" for tool in config["run"] for dbconfig in config["run"][tool][wildcards.database] for parameter in config["run"][tool][wildcards.database][dbconfig]["parameters"]],
			gt = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["gt"]),
			nodes = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["nodes"])
	output: gt_bioboxes = temp("{database}/{sample}/gt.bioboxes.txt"),
			amber_outdir = directory("{database}/{sample}/amber/")
	log: "{database}/{sample}/amber.log"
	params: tools_labels = lambda wildcards: ",".join([tool +"-"+ dbconfig +"-"+ parameter for tool in config["run"] for dbconfig in config["run"][tool][wildcards.database] for parameter in config["run"][tool][wildcards.database][dbconfig]["parameters"]]),
			results_tmp = lambda wildcards: [wildcards.database +"/"+ wildcards.sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".amber" for tool in config["run"] for dbconfig in config["run"][tool][wildcards.database] for parameter in config["run"][tool][wildcards.database][dbconfig]["parameters"]],
	conda: srcdir("../envs/amber.yaml")
	shell: 
		"""
		mkdir -p {output.amber_outdir}

		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n\\n@@SEQUENCEID\\tBINID\\tTAXID\\t_LENGTH\\n" > {output.gt_bioboxes}

		# get read lenghts
		join <(sort -k 1,1 {input.gt}) <(zcat {input.fq1} | sed -n '1~4p;2~4p' | paste - - | awk 'FS="\\t"{{print substr($1,2,length($1)-3)"\\t"length($2)}}' | sort -k 1,1) -t$'\\t' -o "1.1,1.2,1.3,2.2" >> {output.gt_bioboxes}
	
		# ambers just accepts plain text
		for file in {input.results}
		do
		    gzip -d -c ${{file}} > ${{file%.out.gz}}.amber
		done

		# run amber
		{config[amber_dir]}/amber.py -g {output.gt_bioboxes} --ncbi_nodes_file {input.nodes} --output_dir {output.amber_outdir} -l {params.tools_labels} {params.results_tmp} >> {log} 2>&1

		# remove temp files
		rm {params.results_tmp}
		"""

rule evaluation:
	input: results = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.out.gz",
			gt = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["gt"]),
			nodes = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["nodes"]),
			merged = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["merged"]),
			acc_len_taxid_assembly = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["acc_len_taxid_assembly"])
	output: cumu_tsv = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.cumu.tsv",
			cumu_npz = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.cumu.npz",
			rank_tsv = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.rank.tsv",
			rank_npz = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.rank.npz"
	log: "{database}/{sample}/{tool}-{dbconfig}-{parameters}.eval.log"
	shell: "python3 {config[scripts_dir]}eval.py -i {input.results} -g  {input.gt} -d {input.acc_len_taxid_assembly} -n {input.nodes} -m {input.merged} --output-tab-cumulative {output.cumu_tsv} --output-npz-cumulative {output.cumu_npz} --output-tab-rank {output.rank_tsv} --output-npz-rank {output.rank_npz} > {log} 2>&1"

rule ganon:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["ganon"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: out="{database}/{sample}/ganon-{dbconfig}-{parameters}.out",
			lca="{database}/{sample}/ganon-{dbconfig}-{parameters}.lca",
			rep="{database}/{sample}/ganon-{dbconfig}-{parameters}.rep",
			tre="{database}/{sample}/ganon-{dbconfig}-{parameters}.tre",
			time="{database}/{sample}/ganon-{dbconfig}-{parameters}.time",
			mbpm=temp("{database}/{sample}/ganon-{dbconfig}-{parameters}.mbpm")
	log: "{database}/{sample}/ganon-{dbconfig}-{parameters}.log"
	threads: config["threads"]
	conda: srcdir("../envs/ganon.yaml")
	params: outprefix = "{database}/{sample}/ganon-{dbconfig}-{parameters}",
			input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if "fq2" in config["samples"][wildcards.sample] else "",
			params = lambda wildcards: config["run"]["ganon"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters],
	shell: 
		"""
		/usr/bin/time -v --output={output.time} {config[tools_path][ganon]}ganon classify --verbose -d {input.dbprefix}/ganon_db -r {input.fq1} -t {threads} {params.params} -o {params.outprefix} > {log} 2>&1

		grep "ganon-classify processed " {log} | grep -o "[0-9.]* Mbp/m" | sed 's/ Mbp\\/m//g' > {output.mbpm}
		"""

rule ganon_format:
	input: lca="{database}/{sample}/ganon-{dbconfig}-{parameters}.lca",
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["ganon"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: "{database}/{sample}/ganon-{dbconfig}-{parameters}.out.gz"
	shell: 
		"""
		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n@@SEQUENCEID\\tBINID\\tTAXID\\n" | gzip > {output}
		
		# if numeric (taxid) or assembly
		awk 'FS="\\t"{{if($2 ~ /^[0-9]+$/){{print substr($1,1,length($1)-2)"\\t0\\t"$2}}}}' {input.lca} | gzip >> {output}
		
		# get taxid if classified at assembly level
		join -1 2 -2 2 <(awk 'FS="\\t"{{if( $2 !~ /^[0-9]+$/){{print substr($1,1,length($1)-2)"\\t"$2"\\t"$3}}}}' {input.lca} | sort -k 2,2) <(cut -f 3,4 {input.dbprefix}/ganon_db.bins | sort | uniq | sort -k 2,2) -t$'\\t' -o "1.1,1.2,2.1" | gzip >> {output}
		"""

rule krakenuniq:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["krakenuniq"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: res = "{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.res",
			report = "{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.report",
			time="{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.time",
			mbpm=temp("{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.mbpm")
	log: "{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.log"
	threads: config["threads"]
	conda: srcdir("../envs/krakenuniq.yaml")
	params: input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if "fq2" in config["samples"][wildcards.sample] else "",
			params = lambda wildcards: config["run"]["krakenuniq"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters]
	shell: 
		"""
		/usr/bin/time -v --output={output.time} {config[tools_path][krakenuniq]}krakenuniq --db {input.dbprefix} --threads {threads} --output {output.res} --only-classified-output --fastq-input --gzip-compressed --report-file {output.report} {params.params} {input.fq1} > {log} 2>&1

		grep " processed in " {log} | grep -o "[0-9.]* Mbp/m" | sed 's/ Mbp\\/m//g' > {output.mbpm}
		"""

rule krakenuniq_format:
	input: res = "{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.res",
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["krakenuniq"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: "{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.out.gz"
	shell:
		"""
		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n@@SEQUENCEID\\tBINID\\tTAXID\\n" | gzip > {output}
	
		# get assembly assignemnts:
		join -1 2 -2 1 <(grep "^C" {input.res} | awk 'FS="\\t"{{if($3>=1000000000) print substr($2,1,length($2)-2)"\\t"$3}}' | sort -k 2,2) <(sort -k 1,1 {input.dbprefix}/taxDB) -t$'\\t' -o "1.1,2.3,2.2" | gzip >> {output}

		# append taxid only assignments:
		grep "^C" {input.res} | awk 'FS="\\t"{{if($3<1000000000) print substr($2,1,length($2)-2)"\\t0\\t"$3}}' | gzip >> {output}
		"""

rule kraken:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["kraken"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: res = "{database}/{sample}/kraken-{dbconfig}-{parameters}.res",
			time = "{database}/{sample}/kraken-{dbconfig}-{parameters}.time",
			mbpm=temp("{database}/{sample}/kraken-{dbconfig}-{parameters}.mbpm")
	log: "{database}/{sample}/kraken-{dbconfig}-{parameters}.log"
	threads: config["threads"]
	conda: srcdir("../envs/kraken.yaml")
	params: input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if "fq2" in config["samples"][wildcards.sample] else "",
			params = lambda wildcards: config["run"]["kraken"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters]
	shell: 
		"""
		/usr/bin/time -v --output={output.time} {config[tools_path][kraken]}kraken --db {input.dbprefix} --threads {threads} --output {output.res} --only-classified-output --fastq-input --gzip-compressed {params.params} {input.fq1} > {log} 2>&1

		grep " processed in " {log} | grep -o "[0-9.]* Mbp/m" | sed 's/ Mbp\\/m//g' > {output.mbpm}
		"""

rule kraken_format:
	input: res = "{database}/{sample}/kraken-{dbconfig}-{parameters}.res"
	output: "{database}/{sample}/kraken-{dbconfig}-{parameters}.out.gz"
	shell:
		"""
		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n@@SEQUENCEID\\tBINID\\tTAXID\\n" | gzip > {output}

		grep "^C" {input.res} | awk 'FS="\\t"{{print substr($2,1,length($2)-2)"\\t0\\t"$3}}' | gzip >> {output}
		"""

rule centrifuge:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["centrifuge"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: res = "{database}/{sample}/centrifuge-{dbconfig}-{parameters}.res",
			time = "{database}/{sample}/centrifuge-{dbconfig}-{parameters}.time",
			mbpm=temp("{database}/{sample}/centrifuge-{dbconfig}-{parameters}.mbpm")
	log: "{database}/{sample}/centrifuge-{dbconfig}-{parameters}.log"
	threads: config["threads"]
	conda: srcdir("../envs/centrifuge.yaml")
	params: input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if "fq2" in config["samples"][wildcards.sample] else "",
			params = lambda wildcards: config["run"]["centrifuge"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters]
	shell: 
		"""
		/usr/bin/time -v --output={output.time} {config[tools_path][centrifuge]}centrifuge -t -x {input.dbprefix}/centrifuge_db -U {input.fq1} --threads {threads} -S {output.res} --report-file /dev/null {params.params} > {log} 2>&1

		grep -oP "(?<=Time searching: ).*" {log} | awk '{{split($0,a,":");sec=(a[1]*3600)+(a[2]*60)+a[3]; print "*"sec"*"}}' > {output.mbpm}
		"""

rule centrifuge_format:
	input: res= "{database}/{sample}/centrifuge-{dbconfig}-{parameters}.res",
			nodes = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["nodes"]),
			acc_len_taxid_assembly = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["acc_len_taxid_assembly"])
	output: parsed = temp("{database}/{sample}/centrifuge-{dbconfig}-{parameters}.parsed"),
			final = "{database}/{sample}/centrifuge-{dbconfig}-{parameters}.out.gz"
	params: params = lambda wildcards: config["run"]["centrifuge"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters]
	shell: 
		"""
		# group results by taxid
		if [ "{params.params}" = "taxid" ]; then
			awk -F"\\t" 'NR>1 {{if($2 != "unclassified" && $3 != "0"){{print $1"\\t0\\t"$3}}}}' {input.res} | sort | uniq > {output.parsed}
		else
			awk -F"\\t" -v acc_len_taxid_assembly="{input.acc_len_taxid_assembly}" '
			BEGIN{{while (getline < acc_len_taxid_assembly){{acc2assembly[$1]=$4;}}; close(acc_len_taxid_assembly)}} 
			NR>1 {{if($2 != "unclassified" && $3 != "0"){{a=($2 in acc2assembly)?acc2assembly[$2]:"0";print $1"\\t"a"\\t"$3}}}}' {input.res} | sort | uniq > {output.parsed}
		fi

		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n@@SEQUENCEID\\tBINID\\tTAXID\\n" | gzip > {output.final}

		python3 {config[scripts_dir]}/centrifuge_lca.py {output.parsed} {input.nodes} | gzip >> {output.final}
		"""


rule diamond:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["diamond"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: res = "{database}/{sample}/diamond-{dbconfig}-{parameters}.res",
			time = "{database}/{sample}/diamond-{dbconfig}-{parameters}.time",
			mbpm=temp("{database}/{sample}/diamond-{dbconfig}-{parameters}.mbpm")
	log: "{database}/{sample}/diamond-{dbconfig}-{parameters}.log"
	threads: config["threads"]
	conda: srcdir("../envs/diamond.yaml")
	params: input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if "fq2" in config["samples"][wildcards.sample] else "",
			params = lambda wildcards: config["run"]["diamond"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters]
	shell: 
		"""
		/usr/bin/time -v --output={output.time} {config[tools_path][diamond]}diamond blastx --db {input.dbprefix}/diamond_db --query {input.fq1} --out {output.res} --outfmt 102 --threads {threads} {params.params} > {log} 2>&1

		grep -oP "(?<=Total time \\= ).*" {log} | awk '{{print "*"substr($1, 1, length($1)-1)"*"}}' > {output.mbpm}
		"""

rule diamond_format:
	input: res = "{database}/{sample}/diamond-{dbconfig}-{parameters}.res"
	output: "{database}/{sample}/diamond-{dbconfig}-{parameters}.out.gz"
	shell:
		"""
		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n@@SEQUENCEID\\tBINID\\tTAXID\\n" | gzip > {output}

		awk 'FS="\\t"{{if($2!="0"){{print substr($1,1,length($1)-2)"\\t0\\t"$2;}} }}' {input.res} | gzip >> {output}
		"""


rule clark:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["clark"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: csv = "{database}/{sample}/clark-{dbconfig}-{parameters}.csv",
			fq1 = temp("{database}/{sample}/clark-{dbconfig}-{parameters}.fq"),
			time = "{database}/{sample}/clark-{dbconfig}-{parameters}.time",
			mbpm=temp("{database}/{sample}/clark-{dbconfig}-{parameters}.mbpm")
	log: "{database}/{sample}/clark-{dbconfig}-{parameters}.log"
	threads: config["threads"]
	conda: srcdir("../envs/clark.yaml")
	params: outprefix = "{database}/{sample}/clark-{dbconfig}-{parameters}",
			input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if "fq2" in config["samples"][wildcards.sample] else "",
			params = lambda wildcards: config["run"]["clark"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters]
	shell: 
		"""
		/usr/bin/time -v --output={output.time} bash -c "
		gzip -d -c {input.fq1} > {output.fq1};
		{config[tools_path][clark]}CLARK -T {input.dbprefix}/targets.txt -D {input.dbprefix}/custom_0/ -R {params.outprefix} -O {output.fq1} -n {threads} {params.params} > {log} 2>&1;
		"

		grep -oP "(?<= - Assignment time\: )[^s]*" {log} | awk '{{print "*"$1"*"}}' > {output.mbpm}
		"""

rule clark_format:
	input: csv = "{database}/{sample}/clark-{dbconfig}-{parameters}.csv",
	output: "{database}/{sample}/clark-{dbconfig}-{parameters}.out.gz"
	shell:
		"""
		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n@@SEQUENCEID\\tBINID\\tTAXID\\n" | gzip > {output}

		awk 'FS=","{{if(NR>=2 && $3!="NA") print substr($1,1,length($1)-2)"\\t0\\t"$3;}}' {input.csv} | gzip >> {output}
		"""
