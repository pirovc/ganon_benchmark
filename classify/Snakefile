workdir: config["workdir"]
def remove_clusterlog(): shell('if [ -d "clusterlog/" ]; then if [ ! "$(ls -A clusterlog/)" ]; then rm -rf clusterlog/; fi; fi')
onstart:
        shell("mkdir -p clusterlog/")
onsuccess:
        remove_clusterlog()
onerror:
        remove_clusterlog()

rule all:
	input:
		cumu_tsv = [database +"/"+ sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".cumu.tsv" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for dbconfig in config["run"][tool][database] for parameter in config["run"][tool][database][dbconfig]["parameters"] for sample in config["samples"]],
		cumu_npz = [database +"/"+ sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".cumu.npz" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for dbconfig in config["run"][tool][database] for parameter in config["run"][tool][database][dbconfig]["parameters"] for sample in config["samples"]],
		rank_tsv = [database +"/"+ sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".rank.tsv" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for dbconfig in config["run"][tool][database] for parameter in config["run"][tool][database][dbconfig]["parameters"] for sample in config["samples"]],
		rank_npz = [database +"/"+ sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".rank.npz" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for dbconfig in config["run"][tool][database] for parameter in config["run"][tool][database][dbconfig]["parameters"] for sample in config["samples"]],
		stats = [database +"/"+ sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".stats" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for dbconfig in config["run"][tool][database] for parameter in config["run"][tool][database][dbconfig]["parameters"] for sample in config["samples"]],
		bioboxes = [database +"/"+ sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".bioboxes.gz" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for dbconfig in config["run"][tool][database] for parameter in config["run"][tool][database][dbconfig]["parameters"] for sample in config["samples"]],
		# integrated results as set to remove repeated entries
		amber_output = {database +"/"+ sample +"/amber/index.html" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for sample in config["samples"]},
		plot_cumu = {database +"/"+ sample +"/plot_cumu.png" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for sample in config["samples"]},
		plot_rank = {database +"/"+ sample +"/plot_rank.png" for tool in config["run"] for database in config["run"][tool] if database in config["databases"] for sample in config["samples"]}

rule plots:
	input: cumu_npz = lambda wildcards: [wildcards.database +"/"+ wildcards.sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".cumu.npz" for tool in config["run"] if wildcards.database in config["run"][tool] for dbconfig in config["run"][tool][wildcards.database] for parameter in config["run"][tool][wildcards.database][dbconfig]["parameters"]],
		   rank_npz = lambda wildcards: [wildcards.database +"/"+ wildcards.sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".rank.npz" for tool in config["run"] if wildcards.database in config["run"][tool] for dbconfig in config["run"][tool][wildcards.database] for parameter in config["run"][tool][wildcards.database][dbconfig]["parameters"]]
	output: plot_cumu = "{database}/{sample}/plot_cumu.png",
			plot_rank = "{database}/{sample}/plot_rank.png"
	params: scripts_path = srcdir("../scripts/"),
			tools_labels = lambda wildcards: " ".join(["\"" + tool +" ("+ dbconfig +"-"+ parameter + ")\"" for tool in config["run"] if wildcards.database in config["run"][tool] for dbconfig in config["run"][tool][wildcards.database] for parameter in config["run"][tool][wildcards.database][dbconfig]["parameters"]])
	conda: srcdir("../envs/plots.yaml")
	shell:
		"""
		{params.scripts_path}plots.py -i {input.cumu_npz} -l {params.tools_labels} -o {output.plot_cumu}
		{params.scripts_path}plots.py -i {input.rank_npz} -l {params.tools_labels} -o {output.plot_rank}
		"""

rule gzip_bioboxes: 
	input: amber_output = "{database}/{sample}/amber/index.html", # only after amber is finished
			bioboxes = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.bioboxes"
	output: "{database}/{sample}/{tool}-{dbconfig}-{parameters}.bioboxes.gz"
	shell: "gzip {input.bioboxes}"

rule stats:
	input: time = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.time",
			mbpm = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.mbpm"
	output: stats = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.stats"
	shell: 
		"""
		#######
		# .stats: tool <tab> sample <tab> database <tab> dbconfig <tab> parameters <tab> time elapsed <tab> time elapsed seconds <tab> Mbp/m <tab> peak memory bytes
		######

		# elapsed time
		timeelapsed=$(grep -oP "(?<=Elapsed \\(wall clock\\) time \\(h:mm:ss or m:ss\\): ).+" {input.time})
		# time in seconds
		timesec=$(echo ${{timeelapsed}} | awk '{{l=split($0,a,":");if(l==2){{sec=(a[1]*60)+a[2]}}else{{sec=(a[1]*3600)+(a[2]*60)+a[3]}};print sec }}')

		# peak memory in kilobytes 
		memkbytes=$(grep -oP "(?<=Maximum resident set size \\(kbytes\\): ).+" {input.time})
		# peak memory in bytes
		membytes=$((memkbytes*1000))

		#Mbp/m
		mbpm=$(cat {input.mbpm})

		echo "{wildcards.tool}\t{wildcards.sample}\t{wildcards.database}\t{wildcards.dbconfig}\t{wildcards.parameters}\t${{timeelapsed}}\t${{timesec}}\t${{mbpm}}\t${{membytes}}" > {output.stats}
		"""

# get latest taxdump to evaluate all tools equally in case they differ in their taxonomy versions
rule taxdump_eval:
	output: eval_nodes = "{database}/{sample}/eval_nodes.dmp",
			eval_merged = "{database}/{sample}/eval_merged.dmp",
			eval_names = "{database}/{sample}/eval_names.dmp",
			taxdump = temp("{database}/{sample}/taxdump.tar.gz")
	params: outprefix = "{database}/{sample}/"
	shell: 
		"""
		curl -L --silent -o {output.taxdump}  ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz
		tar xf {output.taxdump} -C {params.outprefix} nodes.dmp names.dmp merged.dmp
		mv {params.outprefix}nodes.dmp {params.outprefix}eval_nodes.dmp
		mv {params.outprefix}merged.dmp {params.outprefix}eval_merged.dmp
		mv {params.outprefix}names.dmp {params.outprefix}eval_names.dmp
		"""

rule amber_gt:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			gt = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["gt"]),
	output: gt_bioboxes = "{database}/{sample}/amber.gt.bioboxes"
	shell: 
		"""
		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n\\n@@SEQUENCEID\\tBINID\\tTAXID\\t_LENGTH\\n" > {output.gt_bioboxes}

		# get read lenghts
		join <(sort -k 1,1 {input.gt}) <(zcat {input.fq1} | sed -n '1~4p;2~4p' | paste - - | awk 'FS="\\t"{{print substr($1,2,length($1)-3)"\\t"length($2)}}' | sort -k 1,1) -t$'\\t' -o "1.1,1.2,1.3,2.2" >> {output.gt_bioboxes}
		"""

rule amber:
	input: 	bioboxes = lambda wildcards: [wildcards.database +"/"+ wildcards.sample +"/"+ tool +"-"+ dbconfig +"-"+ parameter + ".bioboxes" for tool in config["run"] if wildcards.database in config["run"][tool] for dbconfig in config["run"][tool][wildcards.database] for parameter in config["run"][tool][wildcards.database][dbconfig]["parameters"]],
			eval_nodes = "{database}/{sample}/eval_nodes.dmp",
			eval_merged = "{database}/{sample}/eval_merged.dmp",
			eval_names = "{database}/{sample}/eval_names.dmp",
			gt_bioboxes = "{database}/{sample}/amber.gt.bioboxes",
	output: amber_output = "{database}/{sample}/amber/index.html"
	log: "{database}/{sample}/amber.log"
	params: amber_dir = lambda wildcards: [wildcards.database +"/"+ wildcards.sample + "/amber/"],
			tools_labels = lambda wildcards: ",".join([tool +"-"+ dbconfig +"-"+ parameter for tool in config["run"] if wildcards.database in config["run"][tool] for dbconfig in config["run"][tool][wildcards.database] for parameter in config["run"][tool][wildcards.database][dbconfig]["parameters"]])
	conda: srcdir("../envs/amber.yaml")
	shell: 
		"""
		####################################
		## TO RE-RUN AMBER WITH NEW FILES ##
		# gzip -d *.bioboxes.gz
		# touch *.tsv *.npz
		####################################

		{config[tools_path][amber]}amber.py -g {input.gt_bioboxes} --ncbi_nodes_file {input.eval_nodes} --ncbi_merged_file {input.eval_merged} --ncbi_names_file {input.eval_names} --output_dir {params.amber_dir} -l {params.tools_labels} {input.bioboxes} >> {log} 2>&1
		"""

rule evaluation:
	input: results = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.bioboxes",
			gt = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["gt"]),
			eval_nodes = "{database}/{sample}/eval_nodes.dmp",
			eval_merged = "{database}/{sample}/eval_merged.dmp",
			acc_len_taxid_assembly = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["acc_len_taxid_assembly"])
	output: cumu_tsv = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.cumu.tsv",
			cumu_npz = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.cumu.npz",
			rank_tsv = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.rank.tsv",
			rank_npz = "{database}/{sample}/{tool}-{dbconfig}-{parameters}.rank.npz"
	conda: srcdir("../envs/eval.yaml")
	params: scripts_path = srcdir("../scripts/")
	log: "{database}/{sample}/{tool}-{dbconfig}-{parameters}.eval.log"
	shell: "python3 {params.scripts_path}eval.py -i {input.results} -g  {input.gt} -d {input.acc_len_taxid_assembly} -n {input.eval_nodes} -m {input.eval_merged} --output-tab-cumulative {output.cumu_tsv} --output-npz-cumulative {output.cumu_npz} --output-tab-rank {output.rank_tsv} --output-npz-rank {output.rank_npz} > {log} 2>&1"

rule ganon:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["ganon"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: lca=temp("{database}/{sample}/ganon-{dbconfig}-{parameters}.lca"),
			rep=temp("{database}/{sample}/ganon-{dbconfig}-{parameters}.rep"),
			tre=temp("{database}/{sample}/ganon-{dbconfig}-{parameters}.tre"),
			time="{database}/{sample}/ganon-{dbconfig}-{parameters}.time",
			mbpm=temp("{database}/{sample}/ganon-{dbconfig}-{parameters}.mbpm")
	log: "{database}/{sample}/ganon-{dbconfig}-{parameters}.log"
	threads: config["threads"]
	conda: srcdir("../envs/ganon.yaml")
	params: outprefix = "{database}/{sample}/ganon-{dbconfig}-{parameters}",
			input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if config["samples"][wildcards.sample]["fq2"] else "",
			params = lambda wildcards: config["run"]["ganon"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters],
	shell: 
		"""
		if [[ -z "{params.input_fq2}" ]]; then # single-end
			/usr/bin/time -v --output={output.time} {config[tools_path][ganon]}ganon classify --verbose --db-prefix {params.dbprefix}/ganon_db -r {input.fq1} -t {threads} {params.params} -o {params.outprefix} > {log} 2>&1
		else # paired-end
			/usr/bin/time -v --output={output.time} {config[tools_path][ganon]}ganon classify --verbose --db-prefix {params.dbprefix}/ganon_db -p {input.fq1} {params.input_fq2} -t {threads} {params.params} -o {params.outprefix} > {log} 2>&1		
		fi
		grep "ganon-classify processed " {log} | grep -o "[0-9.]* Mbp/m" | sed 's/ Mbp\\/m//g' > {output.mbpm}
		"""

rule ganon_format:
	input: lca="{database}/{sample}/ganon-{dbconfig}-{parameters}.lca",
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["ganon"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: bioboxes = "{database}/{sample}/ganon-{dbconfig}-{parameters}.bioboxes",
			bins = temp("{database}/{sample}/ganon-{dbconfig}-{parameters}.bins")
	shell: 
		"""
		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n@@SEQUENCEID\\tBINID\\tTAXID\\n" > {output.bioboxes}
		
		# if numeric (taxid) or assembly
		awk 'FS="\\t"{{if($2 ~ /^[0-9]+$/){{print substr($1,1,length($1)-2)"\\t0\\t"$2}}}}' {input.lca} >> {output.bioboxes}
		
		# regenerate bins
		python -c 'import pickle, gzip;print(*(line for line in pickle.load(gzip.open("{input.dbprefix}/ganon_db.gnn", "rb"))["bins"]), sep="\\n")' > {output.bins}

		# get taxid if classified at assembly level
		join -1 2 -2 2 \
		<(awk 'FS="\\t"{{if( $2 !~ /^[0-9]+$/){{print substr($1,1,length($1)-2)"\\t"$2"\\t"$3}}}}' {input.lca} | sort -k 2,2) \
		<(cut -f 3,4 {output.bins} | sort | uniq | sort -k 2,2) \
		-t$'\\t' -o "1.1,1.2,2.1" >> {output.bioboxes}
		"""

rule krakenuniq:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["krakenuniq"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: res = temp("{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.res"),
			report = temp("{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.report"),
			time="{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.time",
			mbpm=temp("{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.mbpm")
	log: "{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.log"
	threads: config["threads"]
	conda: srcdir("../envs/krakenuniq.yaml")
	params: input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if config["samples"][wildcards.sample]["fq2"] else "",
			params = lambda wildcards: config["run"]["krakenuniq"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters]
	shell: 
		"""
		if [[ -z "{params.input_fq2}" ]]; then # single-end
			/usr/bin/time -v --output={output.time} {config[tools_path][krakenuniq]}krakenuniq --db {input.dbprefix} --threads {threads} --output {output.res} --only-classified-output --fastq-input --gzip-compressed --report-file {output.report} {params.params} {input.fq1} > {log} 2>&1
		else # paired-end
			/usr/bin/time -v --output={output.time} {config[tools_path][krakenuniq]}krakenuniq --db {input.dbprefix} --threads {threads} --output {output.res} --only-classified-output --fastq-input --gzip-compressed --report-file {output.report} {params.params} --paired {input.fq1} {params.input_fq2} > {log} 2>&1
		fi
		grep " processed in " {log} | grep -o "[0-9.]* Mbp/m" | sed 's/ Mbp\\/m//g' > {output.mbpm}
		"""

rule krakenuniq_format:
	input: res = "{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.res",
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["krakenuniq"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: "{database}/{sample}/krakenuniq-{dbconfig}-{parameters}.bioboxes"
	params: input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if config["samples"][wildcards.sample]["fq2"] else ""
	shell:
		"""
		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n@@SEQUENCEID\\tBINID\\tTAXID\\n" > {output}
	
		# output header changes when sinle or paired (/1 or nothing)
		if [[ -z "{params.input_fq2}" ]]; then # single-end
			header_suffix=2;
		else # paired-end
			header_suffix=0;
		fi

		# get assembly assignemnts:
		join -1 2 -2 1 <(grep "^C" {input.res} | awk -v header_suffix="${{header_suffix}}" 'FS="\\t"{{if($3>=1000000000) print substr($2,1,length($2)-header_suffix)"\\t"$3}}' | sort -k 2,2) <(sort -k 1,1 {input.dbprefix}/taxDB) -t$'\\t' -o "1.1,2.3,2.2" >> {output}

		# append taxid only assignments:
		grep "^C" {input.res} | awk -v header_suffix="${{header_suffix}}" 'FS="\\t"{{if($3<1000000000) print substr($2,1,length($2)-header_suffix)"\\t0\\t"$3}}' >> {output}
		"""

rule kraken:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["kraken"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: res = temp("{database}/{sample}/kraken-{dbconfig}-{parameters}.res"),
			time = "{database}/{sample}/kraken-{dbconfig}-{parameters}.time",
			mbpm=temp("{database}/{sample}/kraken-{dbconfig}-{parameters}.mbpm")
	log: "{database}/{sample}/kraken-{dbconfig}-{parameters}.log"
	threads: config["threads"]
	conda: srcdir("../envs/kraken.yaml")
	params: input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if config["samples"][wildcards.sample]["fq2"] else "",
			params = lambda wildcards: config["run"]["kraken"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters]
	shell: 
		"""
		if [[ -z "{params.input_fq2}" ]]; then # single-end
			/usr/bin/time -v --output={output.time} {config[tools_path][kraken]}kraken --db {input.dbprefix} --threads {threads} --output {output.res} --only-classified-output --fastq-input --gzip-compressed {params.params} {input.fq1} > {log} 2>&1
		else # paired-end
			/usr/bin/time -v --output={output.time} {config[tools_path][kraken]}kraken --db {input.dbprefix} --threads {threads} --output {output.res} --only-classified-output --fastq-input --gzip-compressed {params.params} --paired {input.fq1} {params.input_fq2} > {log} 2>&1
		fi

		grep " processed in " {log} | grep -o "[0-9.]* Mbp/m" | sed 's/ Mbp\\/m//g' > {output.mbpm}
		"""

rule kraken_format:
	input: res = "{database}/{sample}/kraken-{dbconfig}-{parameters}.res"
	output: "{database}/{sample}/kraken-{dbconfig}-{parameters}.bioboxes"
	params: input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if config["samples"][wildcards.sample]["fq2"] else ""
	shell:
		"""
		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n@@SEQUENCEID\\tBINID\\tTAXID\\n" > {output}

		# output header changes when sinle or paired (/1 or nothing)
		if [[ -z "{params.input_fq2}" ]]; then # single-end
			header_suffix=2;
		else # paired-end
			header_suffix=0;
		fi

		grep "^C" {input.res} | awk -v header_suffix="${{header_suffix}}" 'FS="\\t"{{print substr($2,1,length($2)-header_suffix)"\\t0\\t"$3}}' >> {output}
		"""

rule centrifuge:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["centrifuge"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: res = temp("{database}/{sample}/centrifuge-{dbconfig}-{parameters}.res"),
			time = "{database}/{sample}/centrifuge-{dbconfig}-{parameters}.time",
			mbpm=temp("{database}/{sample}/centrifuge-{dbconfig}-{parameters}.mbpm")
	log: "{database}/{sample}/centrifuge-{dbconfig}-{parameters}.log"
	conda: srcdir("../envs/centrifuge.yaml")
	threads: config["threads"]
	params: input_fq2 = lambda wildcards: config["samples"][wildcards.sample]["fq2"] if config["samples"][wildcards.sample]["fq2"] else "",
			params = lambda wildcards: config["run"]["centrifuge"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters] if config["run"]["centrifuge"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters] != "taxid" else ""
	shell: 
		"""
		if [[ -z "{params.input_fq2}" ]]; then # single-end
			/usr/bin/time -v --output={output.time} {config[tools_path][centrifuge]}centrifuge -t -x {input.dbprefix}/centrifuge_db -U {input.fq1} --threads {threads} -S {output.res} --report-file /dev/null {params.params} > {log} 2>&1
		else # paired-end
			/usr/bin/time -v --output={output.time} {config[tools_path][centrifuge]}centrifuge -t -x {input.dbprefix}/centrifuge_db -1 {input.fq1} -2 {params.input_fq2} --threads {threads} -S {output.res} --report-file /dev/null {params.params} > {log} 2>&1		
		fi
		grep -oP "(?<=Time searching: ).*" {log} | awk '{{split($0,a,":");sec=(a[1]*3600)+(a[2]*60)+a[3]; print "*"sec"*"}}' > {output.mbpm}
		"""

rule centrifuge_format:
	input: res= "{database}/{sample}/centrifuge-{dbconfig}-{parameters}.res",
			eval_nodes = "{database}/{sample}/eval_nodes.dmp",
			eval_merged = "{database}/{sample}/eval_merged.dmp",
			acc_len_taxid_assembly = lambda wildcards: os.path.abspath(config["databases"][wildcards.database]["acc_len_taxid_assembly"])
	output: parsed = temp("{database}/{sample}/centrifuge-{dbconfig}-{parameters}.parsed"),
			final = "{database}/{sample}/centrifuge-{dbconfig}-{parameters}.bioboxes"
	conda: srcdir("../envs/eval.yaml")
	params: params = lambda wildcards: config["run"]["centrifuge"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters],
			scripts_path = srcdir("../scripts/")
	shell: 
		"""
		# group results by taxid
		if [ "{params.params}" = "taxid" ]; then
			awk -F"\\t" 'NR>1 {{if($2 != "unclassified" && $3 != "0"){{print $1"\\t0\\t"$3}}}}' {input.res} | sort | uniq > {output.parsed}
		else
			awk -F"\\t" -v acc_len_taxid_assembly="{input.acc_len_taxid_assembly}" '
			BEGIN{{while (getline < acc_len_taxid_assembly){{acc2assembly[$1]=$4;}}; close(acc_len_taxid_assembly)}} 
			NR>1 {{if($2 != "unclassified" && $3 != "0"){{a=($2 in acc2assembly)?acc2assembly[$2]:"0";print $1"\\t"a"\\t"$3}}}}' {input.res} | sort | uniq > {output.parsed}
		fi

		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n@@SEQUENCEID\\tBINID\\tTAXID\\n" > {output.final}

		python3 {params.scripts_path}centrifuge_lca.py {output.parsed} {input.eval_nodes} {input.eval_merged} >> {output.final}
		"""


rule diamond:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["diamond"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: res = temp("{database}/{sample}/diamond-{dbconfig}-{parameters}.res"),
			time = "{database}/{sample}/diamond-{dbconfig}-{parameters}.time",
			mbpm= temp("{database}/{sample}/diamond-{dbconfig}-{parameters}.mbpm")
	log: "{database}/{sample}/diamond-{dbconfig}-{parameters}.log"
	threads: config["threads"]
	conda: srcdir("../envs/diamond.yaml")
	params: input_fq2 = lambda wildcards: config["samples"][wildcards.sample]["fq2"] if config["samples"][wildcards.sample]["fq2"] else "",
			params = lambda wildcards: config["run"]["diamond"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters]
	shell: 
		"""
		if [[ ! -z "{params.input_fq2}" ]]; then # paired-end
			echo "DIAMOND does not support paired-end reads, using just first file" > {log};
		fi
		/usr/bin/time -v --output={output.time} {config[tools_path][diamond]}diamond blastx --db {input.dbprefix}/diamond_db --query {input.fq1} --out {output.res} --outfmt 102 --threads {threads} {params.params} >> {log} 2>&1

		grep -oP "(?<=Total time \\= ).*" {log} | awk '{{print "*"substr($1, 1, length($1)-1)"*"}}' > {output.mbpm}
		"""

rule diamond_format:
	input: res = "{database}/{sample}/diamond-{dbconfig}-{parameters}.res"
	output: "{database}/{sample}/diamond-{dbconfig}-{parameters}.bioboxes"
	shell:
		"""
		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n@@SEQUENCEID\\tBINID\\tTAXID\\n" > {output}

		awk 'FS="\\t"{{if($2!="0"){{print substr($1,1,length($1)-2)"\\t0\\t"$2;}} }}' {input.res} >> {output}
		"""


rule clark:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["clark"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: csv = temp("{database}/{sample}/clark-{dbconfig}-{parameters}.csv"),
			fq1 = temp("{database}/{sample}/clark-{dbconfig}-{parameters}.1.fq"),
			fq2 = temp("{database}/{sample}/clark-{dbconfig}-{parameters}.2.fq"),
			time = "{database}/{sample}/clark-{dbconfig}-{parameters}.time",
			mbpm=temp("{database}/{sample}/clark-{dbconfig}-{parameters}.mbpm")
	log: "{database}/{sample}/clark-{dbconfig}-{parameters}.log"
	threads: config["threads"]
	conda: srcdir("../envs/clark.yaml")
	params: outprefix = "{database}/{sample}/clark-{dbconfig}-{parameters}",
			input_fq2 = lambda wildcards: config["samples"][wildcards.sample]["fq2"] if config["samples"][wildcards.sample]["fq2"] else "",
			params = lambda wildcards: config["run"]["clark"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters]
	shell: 
		"""

		if [[ -z "{params.input_fq2}" ]]; then # single-end
			/usr/bin/time -v --output={output.time} bash -c "
			gzip -d -c {input.fq1} > {output.fq1};
			touch {output.fq2};
			{config[tools_path][clark]}CLARK -T {input.dbprefix}/targets.txt -D {input.dbprefix}/custom_0/ -R {params.outprefix} -O {output.fq1} -n {threads} {params.params} > {log} 2>&1;
			"
		else # paired-end
			/usr/bin/time -v --output={output.time} bash -c "
			gzip -d -c {input.fq1} > {output.fq1};
			gzip -d -c {params.input_fq2} > {output.fq2};
			{config[tools_path][clark]}CLARK -T {input.dbprefix}/targets.txt -D {input.dbprefix}/custom_0/ -R {params.outprefix} -P {output.fq1} {output.fq2} -n {threads} {params.params} > {log} 2>&1;
			"
		fi

		grep -oP "(?<= - Assignment time\: )[^s]*" {log} | awk '{{print "*"$1"*"}}' > {output.mbpm}
		"""

rule clark_format:
	input: csv = "{database}/{sample}/clark-{dbconfig}-{parameters}.csv",
	output: "{database}/{sample}/clark-{dbconfig}-{parameters}.bioboxes"
	params: input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if config["samples"][wildcards.sample]["fq2"] else ""
	shell:
		"""
		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n@@SEQUENCEID\\tBINID\\tTAXID\\n" > {output}

		# output header changes when sinle or paired (/1 or nothing)
		if [[ -z "{params.input_fq2}" ]]; then # single-end
			header_suffix=2;
		else # paired-end
			header_suffix=0;
		fi

		awk -v header_suffix="${{header_suffix}}" 'FS=","{{if(NR>=2 && $3!="NA") print substr($1,1,length($1)-header_suffix)"\\t0\\t"$3;}}' {input.csv} >> {output}
		"""

rule kraken2:
	input: fq1 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq1"]),
			dbprefix = lambda wildcards: os.path.abspath(config["run"]["kraken2"][wildcards.database][wildcards.dbconfig]["prefix"])
	output: res = temp("{database}/{sample}/kraken2-{dbconfig}-{parameters}.res"),
			time = "{database}/{sample}/kraken2-{dbconfig}-{parameters}.time",
			mbpm=temp("{database}/{sample}/kraken2-{dbconfig}-{parameters}.mbpm")
	log: "{database}/{sample}/kraken2-{dbconfig}-{parameters}.log"
	threads: config["threads"]
	conda: srcdir("../envs/kraken2.yaml")
	params: input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if config["samples"][wildcards.sample]["fq2"] else "",
			params = lambda wildcards: config["run"]["kraken2"][wildcards.database][wildcards.dbconfig]["parameters"][wildcards.parameters]
	shell: 
		"""
		if [[ -z "{params.input_fq2}" ]]; then # single-end
			/usr/bin/time -v --output={output.time} {config[tools_path][kraken2]}kraken2 --db {input.dbprefix} --threads {threads} --output {output.res} --fastq-input --gzip-compressed {params.params} {input.fq1} > {log} 2>&1
		else # paired-end
			/usr/bin/time -v --output={output.time} {config[tools_path][kraken2]}kraken2 --db {input.dbprefix} --threads {threads} --output {output.res} --fastq-input --gzip-compressed {params.params} --paired {input.fq1} {params.input_fq2} > {log} 2>&1
		fi

		grep " processed in " {log} | grep -o "[0-9.]* Mbp/m" | sed 's/ Mbp\\/m//g' > {output.mbpm}
		"""

rule kraken2_format:
	input: res = "{database}/{sample}/kraken2-{dbconfig}-{parameters}.res"
	output: "{database}/{sample}/kraken2-{dbconfig}-{parameters}.bioboxes"
	params: input_fq2 = lambda wildcards: os.path.abspath(config["samples"][wildcards.sample]["fq2"]) if config["samples"][wildcards.sample]["fq2"] else ""
	shell:
		"""
		# bioboxes header
		printf "@Version:0.9.1\\n@SampleID:{wildcards.sample}\\n@@SEQUENCEID\\tBINID\\tTAXID\\n" > {output}

		# output header changes when sinle or paired (/1 or nothing)
		if [[ -z "{params.input_fq2}" ]]; then # single-end
			header_suffix=2;
		else # paired-end
			header_suffix=0;
		fi

		grep "^C" {input.res} | awk -v header_suffix="${{header_suffix}}" 'FS="\\t"{{print substr($2,1,length($2)-header_suffix)"\\t0\\t"$3}}' >> {output}
		"""
